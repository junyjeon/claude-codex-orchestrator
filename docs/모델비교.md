# Opus 4.6 vs Codex 5.3 비교

2026-02-08 기준. 벤치마크와 커뮤니티 실사용 데이터를 종합했다.


## 벤치마크

| 벤치마크 | Opus 4.6 | Codex 5.3 | 측정 대상 |
|----------|----------|-----------|-----------|
| SWE-Bench Verified | 80.8% | 56.8%* | 실제 버그 수정 |
| Terminal-Bench 2.0 | 65.4% | 77.3% | 터미널 에이전틱 코딩 |
| GPQA Diamond | 77.3% | - | 학술 추론 |
| MMLU Pro | 85.1% | - | 전문 지식 |
| Aider Polyglot | ~72% | ~88% | 다국어 코드 생성 |
| LFG Benchmark | 9.25/10 | 7.5/10 | 복잡한 기능 구현 |

*SWE-Bench 주의: Anthropic은 Verified, OpenAI는 Pro Public 버전을 리포팅한다. 문제 세트가 달라서 직접 비교는 부정확하다.


## 핵심 특성 차이

Opus 4.6은 ceiling이 높고 variance도 높다. 복잡한 작업에서 11개 기능 전부 구현하는 반면, 간단한 작업에서 가끔 "멍청한 실수"를 한다. 깊이 생각하고, 오래 실행하고, 덜 물어본다.

Codex 5.3은 ceiling은 낮지만 variance도 낮다. 결과물이 예측 가능하고 안정적이다. 25% 더 빠르고, 토큰당 비용이 낮다. 상호작용하면서 방향을 잡아주는 스타일에 적합하다.

한마디로: Opus는 자율 주행, Codex는 반자동 주행이다.


## 작업별 최적 모델

### Opus가 확실히 앞서는 영역

아키텍처 설계와 시스템 계획이다. GPQA 77.3%, MMLU Pro 85.1%로 추론 깊이가 다르다. 멀티에이전트 구조 설계, 데이터베이스 스키마, API 설계에서 Codex보다 더 깊은 트레이드오프 분석을 한다.

복잡한 멀티파일 기능 구현이다. LFG 벤치마크에서 Opus 9.25 vs Codex 7.5. Opus가 11개 기능 전부 구현한 반면 Codex는 체크아웃 플로우를 통째로 빠뜨렸다. 여러 파일에 걸친 일관성이 필요한 작업에서 Opus가 강하다.

버그 수정과 디버깅이다. SWE-Bench Verified 80.8%는 실제 오픈소스 프로젝트 버그를 고치는 벤치마크다. 기존 코드를 읽고, 문제를 진단하고, 최소한의 변경으로 수정하는 능력에서 Opus가 압도적이다.

보안 감사와 코드 리뷰다. 추론 깊이가 곧 보안 분석 깊이다. 공격 벡터를 찾고 방어 로직을 설계하는 건 Opus의 영역이다.

한국어 소통이다. 벤치마크 데이터는 없지만 커뮤니티 합의다. 한국어 지시 이해, 한국어 문서 생성에서 Opus가 더 자연스럽다.

### Codex가 확실히 앞서는 영역

터미널 기반 코딩과 스크립팅이다. Terminal-Bench 2.0에서 77.3% vs 65.4%로 12%p 차이. CLI 도구, 셸 스크립트, 터미널 자동화에서 Codex가 더 정확하다.

단일 함수/모듈 코드 생성이다. Aider Polyglot ~88%로 6개 언어(C++, Go, Java, JS, Python, Rust)에서 단일 문제 해결 능력이 높다. 명확하게 정의된 함수 하나를 빠르게 생성할 때 Codex가 효율적이다.

반복적/정형화된 백엔드 구현이다. CRUD 엔드포인트, 미들웨어, 유틸리티 함수처럼 패턴이 명확한 작업에서 Codex의 낮은 variance가 장점이다. 실수가 적고 25% 빠르다.

보일러플레이트 생성이다. 설정 파일, 테스트 스캐폴딩, 타입 정의 같은 반복 작업. 속도가 핵심이고 창의성이 필요 없다.


## 맥락 단절 비용

Codex에 위임하면 CLAUDE.md, rules/, 세션 히스토리가 전달되지 않는다. codex_generate의 context 필드에 Claude가 직접 맥락을 넣어야 하지만, 압축 과정에서 정보가 유실된다. 위임 범위가 넓을수록 품질 저하가 커진다.

Codex의 "낮은 variance"는 장점이 아니라 함정이 될 수 있다. 프로젝트 컨벤션을 모르기 때문에 Codex 자기 스타일로 일관되게 생성한다. 결과물이 안정적으로 보이지만, 프로젝트와 통합하면 스타일 불일치가 드러난다.

codex_review는 예외다. Claude가 리뷰할 코드를 직접 전달하므로 맥락 손실이 없고, 다른 모델의 관점이라는 고유한 가치가 있다.


## 조합 전략

### 위임 판별 기준

"프로젝트 컨벤션 없이도 그대로 쓸 수 있는 코드인가?"

Yes → 위임 가능 (자족적 작업):
- 프로젝트와 무관한 bash/shell 스크립트
- 범용 유틸리티 (sort, hash, parse, format)
- 설정 파일, scaffold 생성
- 1회성 자동화 스크립트

No → 직접 처리:
- 프로젝트 컨벤션이 적용되는 모든 코드
- 기존 코드와 통합되는 기능
- 비즈니스 로직, 상태 관리, UI 컴포넌트
- 버그 수정, 리팩토링, 보안 관련

CRUD 엔드포인트나 테스트 생성은 벤치마크상 Codex가 잘하지만, 실제 프로젝트에서는 컨벤션(네이밍, 에러 처리, 미들웨어 구조)이 필요하므로 직접 처리가 낫다.

### 실전 조합 패턴

가장 가치 있는 조합은 파이프라인이 아니라 교차 리뷰다:

```
1. Opus: 설계 + 구현 + 자체 리뷰 (code-reviewer)
2. Codex: codex_review로 교차 리뷰 (다른 관점)
3. Opus: 리뷰 결과 반영
```

codex_generate는 자족적 작업에만 쓴다:

```
Opus: "이 bash 스크립트 필요해" → codex_generate → Opus: 결과 검증 후 적용
Opus: "범용 파서 함수 필요해" → codex_generate → Opus: 프로젝트 스타일로 조정
```

### 라우팅 규칙 (suggest_model용)

작업을 Codex로 보내야 하는 신호:
- 프로젝트 컨벤션이 불필요한 자족적 코드
- CLI/터미널 스크립트
- 범용 유틸리티 함수 (입출력 스펙이 명확)
- 설정 파일, scaffold 생성

작업을 Opus에 남겨야 하는 신호:
- 프로젝트 컨벤션이 필요한 모든 코드
- 여러 파일에 걸친 기능 구현
- 기존 코드 수정 (버그 수정, 리팩토링)
- 보안 분석, 코드 리뷰
- 한국어 문서 작성


## 출처

- Every.to "The Great Convergence" (2026-02-06)
- OpenAI "Introducing GPT-5.3-Codex" (2026-02-05)
- Anthropic "Introducing Claude Opus 4.6" (2026-02-05)
- NxCode "GPT-5.3 Codex vs Claude Opus 4.6" (2026-02-06)
- VentureBeat "AI Coding Wars" (2026-02-05)
- Hacker News "GPT-5.3-Codex" 스레드
- Aider Polyglot Leaderboard (aider.chat)
- DC Inside 특이점 갤러리 #959833
